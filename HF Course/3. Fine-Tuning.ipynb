{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Предобработка данных",
   "id": "1fd6849a052d7ad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Небольшие примеры",
   "id": "97476618036d1ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Обучим всю модель (тело + голова)",
   "id": "349d3aa271cd08f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:38.897159Z",
     "start_time": "2025-04-03T09:45:37.744505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "old_weights = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "new_weights = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "old_weights['classifier.weight'], new_weights[\n",
    "    'classifier.weight']  # видны изменения в головах, но поменялись не только они, но и веса в теле"
   ],
   "id": "41f7e93cccfc1e53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0138, -0.0222,  0.0421,  ...,  0.0305, -0.0367, -0.0273],\n",
       "         [-0.0140, -0.0306,  0.0452,  ..., -0.0017, -0.0054, -0.0285]]),\n",
       " tensor([[ 0.0138, -0.0222,  0.0411,  ...,  0.0305, -0.0377, -0.0273],\n",
       "         [-0.0140, -0.0306,  0.0462,  ..., -0.0017, -0.0044, -0.0285]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Обучим только голову, заморозив тело",
   "id": "1b4ff1d84fda4916"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:39.918018Z",
     "start_time": "2025-04-03T09:45:38.898156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(\"cuda\")\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "batch[\"labels\"] = torch.tensor([1, 1]).to(\"cuda\")\n",
    "\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3)\n",
    "\n",
    "old_weights = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "model.train()\n",
    "for _ in tqdm(range(100)):  # 100 эпох\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = model(**batch).loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "new_weights = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "old_weights['classifier.weight'], new_weights['classifier.weight']"
   ],
   "id": "3b0c518145fe3458",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      " 22%|██▏       | 22/100 [00:00<00:00, 212.88it/s]\u001B[A\n",
      " 54%|█████▍    | 54/100 [00:00<00:00, 273.45it/s]\u001B[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 295.56it/s][A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0138, -0.0222,  0.0421,  ...,  0.0305, -0.0367, -0.0273],\n",
       "         [-0.0140, -0.0306,  0.0452,  ..., -0.0017, -0.0054, -0.0285]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.0138, -0.0222,  0.0420,  ...,  0.0304, -0.0368, -0.0273],\n",
       "         [-0.0140, -0.0306,  0.0452,  ..., -0.0016, -0.0053, -0.0285]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузка датасета с Hub\n",
   "id": "c26e656ae2aa0aca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.261521Z",
     "start_time": "2025-04-03T09:45:39.919020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "dataset"
   ],
   "id": "4cba96ce65687efe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.276515Z",
     "start_time": "2025-04-03T09:45:46.262538Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.column_names",
   "id": "7593d7ce8556307e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['sentence1', 'sentence2', 'label', 'idx'],\n",
       " 'validation': ['sentence1', 'sentence2', 'label', 'idx'],\n",
       " 'test': ['sentence1', 'sentence2', 'label', 'idx']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.291927Z",
     "start_time": "2025-04-03T09:45:46.278517Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"][:2]",
   "id": "b53caedf05d1bbd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       "  \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\"],\n",
       " 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       "  \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\"],\n",
       " 'label': [1, 0],\n",
       " 'idx': [0, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.308026Z",
     "start_time": "2025-04-03T09:45:46.293053Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"].features",
   "id": "d803674d9f890ff7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Предобработка датасета",
   "id": "2cf145d617f892ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.385970Z",
     "start_time": "2025-04-03T09:45:46.309986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset = tokenizer(dataset[\"train\"][\"sentence1\"], dataset[\"train\"][\"sentence2\"],\n",
    "                              padding=True, truncation=True, )\n",
    "tokenized_dataset.keys()"
   ],
   "id": "747a8ff1f27eca6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Это будет работать только если у нас достаточно оперативной памяти (RAM) для хранения целого датасета во время токенизации (в то время как датасеты из библиотеки Datasets являются Apache Arrow файлами, хранящимися на диске; они будут загружены только в тот момент, когда вы их будете запрашивать).",
   "id": "6e003f3eb3365051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Чтобы хранить данные в формате датасета, мы будем использовать методы Dataset.map(). Это позволит нам сохранить высокую гибкость даже если нам нужно что-то большее, чем просто токенизация. Метод map() работает так: применяет некоторую функцию к каждому элементу датасетаДля данной проблемы есть решение - функция map. Она сохраняет исходные колонки и добавляет новые в зависимости от функции",
   "id": "df0b8d12159544c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.634587Z",
     "start_time": "2025-04-03T09:45:46.386930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function)  # по одной строке за раз\n",
    "tokenized_datasets.column_names"
   ],
   "id": "178359cb0ad6bdef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " 'validation': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " 'test': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**batched=True** ускоряет обработку за счёт групповой токенизации.",
   "id": "18c8f9ae1cef7a9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.883032Z",
     "start_time": "2025-04-03T09:45:46.635582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", truncation=True,\n",
    "                     max_length=128)  # фиксированная длинна после токенизации - 128\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)  # функция вызывается один раз на batch\n",
    "tokenized_datasets.column_names"
   ],
   "id": "27c01ab662c9494f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " 'validation': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask'],\n",
       " 'test': ['sentence1',\n",
       "  'sentence2',\n",
       "  'label',\n",
       "  'idx',\n",
       "  'input_ids',\n",
       "  'token_type_ids',\n",
       "  'attention_mask']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Удалим ненужные столбцы",
   "id": "5f64ef78fd1d790"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.898333Z",
     "start_time": "2025-04-03T09:45:46.884985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets = tokenized_datasets.with_format('torch')\n",
    "\n",
    "tokenized_datasets.column_names"
   ],
   "id": "a092ebc07357e133",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'validation': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       " 'test': ['labels', 'input_ids', 'token_type_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Можно создать датасет меньшего размера, используя **select**",
   "id": "476b41646a561846"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.913516Z",
     "start_time": "2025-04-03T09:45:46.899233Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets['train'].select(range(100))",
   "id": "d762d2fee634e552",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Если надо обрабатывать столбцы как разные",
   "id": "cbbbc73cf52cd06d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.928516Z",
     "start_time": "2025-04-03T09:45:46.914539Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer('zaza', 'berg')",
   "id": "82c3e3297a145ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 195, 10961, 1161, 102, 1129, 10805, 102], 'token_type_ids': [0, 0, 0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:46.944511Z",
     "start_time": "2025-04-03T09:45:46.930419Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(['zaza', 'berg'])",
   "id": "84ad992e6ca9e6ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 195, 10961, 1161, 102], [101, 1129, 10805, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Как можно заметить передача столбцов в тупую в токенайзер не работает",
   "id": "62b673b616a44ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:48.511512Z",
     "start_time": "2025-04-03T09:45:48.377783Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer([dataset[\"train\"][\"sentence1\"], dataset[\"train\"][\"sentence2\"]], padding=True, truncation=True, )",
   "id": "b6047bc0de89eff3",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msentence1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msentence2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2887\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[0;32m   2886\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[1;32m-> 2887\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[0;32m   2888\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2889\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2975\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[0;32m   2970\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2971\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2972\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2973\u001B[0m         )\n\u001B[0;32m   2974\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[1;32m-> 2975\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[0;32m   2976\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m   2977\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   2978\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   2979\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   2980\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   2981\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   2982\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   2983\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   2984\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[0;32m   2985\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   2986\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   2987\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   2988\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   2989\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   2990\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   2991\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   2992\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   2993\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[0;32m   2994\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2995\u001B[0m     )\n\u001B[0;32m   2996\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2997\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[0;32m   2998\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m   2999\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3017\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3018\u001B[0m     )\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3177\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[0;32m   3167\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m   3168\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[0;32m   3169\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   3170\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3175\u001B[0m )\n\u001B[1;32m-> 3177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[0;32m   3178\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m   3179\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   3180\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[0;32m   3181\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   3182\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   3183\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   3184\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   3185\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   3186\u001B[0m     padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[0;32m   3187\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   3188\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   3189\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   3190\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   3191\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   3192\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   3193\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   3194\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   3195\u001B[0m     split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[0;32m   3196\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3197\u001B[0m )\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:539\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m!=\u001B[39m split_special_tokens:\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m=\u001B[39m split_special_tokens\n\u001B[1;32m--> 539\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[0;32m    546\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[0;32m    549\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[0;32m    551\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[0;32m    553\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[0;32m    563\u001B[0m ]\n",
      "\u001B[1;31mTypeError\u001B[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Если не использовать batch, то можно и так, ибо map работает построчно",
   "id": "f5142ed5c9e8f64d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:50.126430Z",
     "start_time": "2025-04-03T09:45:49.313245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer([example[\"sentence1\"], example[\"sentence2\"]], padding=\"max_length\", truncation=True,\n",
    "                     max_length=128)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function)  # по одной строке за раз\n",
    "tokenized_datasets['train']['token_type_ids'][0]"
   ],
   "id": "5a91f3a1d3ac21fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Других способов много, можно спокойно заколхозить",
   "id": "4b1f8cdfe41c39c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dynamic padding\n",
    "\n",
    "Ранее мы использовали фиксированную длину для токенизатора. Чтобы в случае батчинга не заполнять массивы большим количеством пустых значений воспользуемся **DataCollatorWithPadding**"
   ],
   "id": "e272b03ca69ab489"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.482627Z",
     "start_time": "2025-04-03T09:45:51.325121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)  # убираем padding\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
   ],
   "id": "32d935093eea1a29",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.564736Z",
     "start_time": "2025-04-03T09:45:57.482627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True,\n",
    "                              collate_fn=data_collator)\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ],
   "id": "24651c9bcf4e936",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72])\n",
      "torch.Size([16, 78])\n",
      "torch.Size([16, 80])\n",
      "torch.Size([16, 111])\n",
      "torch.Size([16, 80])\n",
      "torch.Size([16, 73])\n",
      "torch.Size([16, 82])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.580509Z",
     "start_time": "2025-04-03T09:45:57.564736Z"
    }
   },
   "cell_type": "code",
   "source": "next(iter(train_dataloader))",
   "id": "ee72858522aea040",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]), 'input_ids': tensor([[  101,  1960,  6449,  ...,     0,     0,     0],\n",
       "        [  101,  1244,  8570,  ...,  1177,   119,   102],\n",
       "        [  101,  1109,  2084,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 13857,  1144,  ...,     0,     0,     0],\n",
       "        [  101,  4180,   118,  ...,     0,     0,     0],\n",
       "        [  101,  1252,  1103,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trainer",
   "id": "d7cc24915a01dd10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Базовая работа",
   "id": "43ee4253130cb8b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.659658Z",
     "start_time": "2025-04-03T09:45:57.580509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test-trainer\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  per_device_train_batch_size=30,\n",
    "                                  per_device_eval_batch_size=30,\n",
    "                                  # eval_strategy='steps',\n",
    "                                  eval_strategy='epoch',\n",
    "                                  learning_rate=1e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  use_cpu=False)"
   ],
   "id": "815b9c3bb9431fd4",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "После создания экземпляра предобученной модели будет распечатано предупреждение. Это происходит потому, что BERT не был предобучен для задачи классификации пар предложений, его последний слой не будет использован, вместо него будет добавлен слой, позволяющий работать с такой задачей. Предупреждения сообщают, что некоторые веса не будут использованы (как раз тех слоев, которые не будут использоваться) и для новых будут инициализированы случайные веса. В заключении предлагается обучить модель, что мы и сделаем прямо сейчас.",
   "id": "b600e67e7a4dc1be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.886923Z",
     "start_time": "2025-04-03T09:45:57.659658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ],
   "id": "9914b1b354894045",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Можно заметить, что к модели автоматически добавился линейный слой в конец",
   "id": "9f787dc60f0678b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:45:57.902675Z",
     "start_time": "2025-04-03T09:45:57.886923Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "68c837d22028a2ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:13.216581Z",
     "start_time": "2025-04-03T09:45:57.902675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "54e4885b103023c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1230' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1230/1230 04:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.449286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.540330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.587870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.610640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.639322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.635914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.647751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1230, training_loss=0.19131156487193535, metrics={'train_runtime': 254.8007, 'train_samples_per_second': 143.956, 'train_steps_per_second': 4.827, 'total_flos': 1562116030111320.0, 'train_loss': 0.19131156487193535, 'epoch': 10.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:16.383834Z",
     "start_time": "2025-04-03T09:50:13.217534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions.predictions.shape, predictions.label_ids.shape"
   ],
   "id": "7b210e6d5fcb0645",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1725, 2), (1725,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:16.398961Z",
     "start_time": "2025-04-03T09:50:16.384843Z"
    }
   },
   "cell_type": "code",
   "source": "predictions.predictions[:5, :]",
   "id": "fb7e183eed08d7a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.4772704,  2.603379 ],\n",
       "       [-2.0506606,  1.9318771],\n",
       "       [-2.4952757,  2.5817323],\n",
       "       [-1.7328042,  1.3866795],\n",
       "       [ 2.3559062, -2.6548834]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:25.783653Z",
     "start_time": "2025-04-03T09:50:25.767816Z"
    }
   },
   "cell_type": "code",
   "source": "predictions.label_ids",
   "id": "812fd5312f045c4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], shape=(1725,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:26.175923Z",
     "start_time": "2025-04-03T09:50:26.159937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import argmax, tensor\n",
    "\n",
    "preds = argmax(tensor(predictions.predictions), dim=-1)\n",
    "preds, preds.shape"
   ],
   "id": "82d721a087618713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1,  ..., 1, 1, 1]), torch.Size([1725]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим на метрики",
   "id": "37aa1d8727be60d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:50:28.349303Z",
     "start_time": "2025-04-03T09:50:27.160717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ],
   "id": "5547abc844beb054",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8289855072463768, 'f1': 0.8735533647663952}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Работа с учетом метрик",
   "id": "aecdd212afa67295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:41.227762Z",
     "start_time": "2025-04-03T09:50:29.474161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import argmax, tensor\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = argmax(tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "3e04fb78d2b1ce01",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\merku\\AppData\\Local\\Temp\\ipykernel_14588\\1530448329.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583257</td>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.850153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.456783</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.872910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.748559</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3969130671881383, metrics={'train_runtime': 130.7957, 'train_samples_per_second': 84.131, 'train_steps_per_second': 10.528, 'total_flos': 419446300011600.0, 'train_loss': 0.3969130671881383, 'epoch': 3.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:46.645088Z",
     "start_time": "2025-04-03T09:52:41.231774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "from torch import argmax, tensor\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = argmax(tensor(predictions.predictions), dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ],
   "id": "eec4b76888ade518",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8220289855072463, 'f1': 0.8723492723492724}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch Training",
   "id": "c066bb1e577d4345"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:56.308712Z",
     "start_time": "2025-04-03T09:52:46.645088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)  # убираем padding\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
   ],
   "id": "1ddf32e65a12ddfd",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:56.324770Z",
     "start_time": "2025-04-03T09:52:56.308712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=16, collate_fn=data_collator, pin_memory=True\n",
    "    # ускорит перекид с CPU на GPU\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=16, collate_fn=data_collator, pin_memory=True\n",
    "    # ускорит перекид с CPU на GPU\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ],
   "id": "7a263892ce4eb282",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([16]),\n",
       " 'input_ids': torch.Size([16, 72]),\n",
       " 'token_type_ids': torch.Size([16, 72]),\n",
       " 'attention_mask': torch.Size([16, 72])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:56.999178Z",
     "start_time": "2025-04-03T09:52:56.324770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "outputs = model(**batch)\n",
    "outputs.loss.item(), outputs.logits.shape"
   ],
   "id": "7bb9988520fd1c00",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8421317338943481, torch.Size([16, 2]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:57.184395Z",
     "start_time": "2025-04-03T09:52:56.999178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ],
   "id": "b0a85d934e5bfc6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:52:57.200194Z",
     "start_time": "2025-04-03T09:52:57.184395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)  # количество эпох * количество батчей\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "num_training_steps"
   ],
   "id": "2fed4fb61df1cef2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:54:16.562604Z",
     "start_time": "2025-04-03T09:52:57.200194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ],
   "id": "aaccdbe8eb39ddcc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/690 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f321306c7d66493fb9692e0334abac82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:54:19.226294Z",
     "start_time": "2025-04-03T09:54:16.564525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ],
   "id": "9b083cc9c41f55de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8529411764705882, 'f1': 0.8969072164948454}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T09:54:25.062439Z",
     "start_time": "2025-04-03T09:54:19.226294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "from torch import argmax, tensor\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = argmax(tensor(predictions.predictions), dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ],
   "id": "4c20c72dcd1b78c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8220289855072463, 'f1': 0.8723492723492724}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ускорение работы с помощью accelerate",
   "id": "4234f2796cd21ab5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T10:11:30.418982Z",
     "start_time": "2025-04-03T10:09:18.742639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from accelerate import Accelerator, notebook_launcher\n",
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "\n",
    "def train_with_accelerator(model):\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dl:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "\n",
    "notebook_launcher(train_with_accelerator, args=(model,), num_processes=1)\n",
    "model"
   ],
   "id": "6bfae6984afbae92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8da2e3317364e719eecd68b73a5ed5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T10:15:10.856914Z",
     "start_time": "2025-04-03T10:15:07.783815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "from torch import inference_mode\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "@inference_mode()\n",
    "def eval_with_accelerator(model,metric):\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dl:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        metric.add_batch(\n",
    "            predictions=accelerator.gather(predictions),\n",
    "            references=accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "\n",
    "\n",
    "notebook_launcher(eval_with_accelerator, args=(model,metric,), num_processes=1)\n",
    "metric.compute()"
   ],
   "id": "7446256ded36a764",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8406862745098039, 'f1': 0.8877374784110535}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
