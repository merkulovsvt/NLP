{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Конвейер",
   "id": "f8350e47988ae378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Предобработка с помощью токенизатора",
   "id": "2082b2877c9665fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:44.618402Z",
     "start_time": "2025-04-02T14:50:44.349577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "inputs = tokenizer(['Wasup, my nigger', 'Hello, my name is Ivan the Berg'], return_tensors='pt', padding=True,\n",
    "                   truncation=True)\n",
    "inputs"
   ],
   "id": "e4d4af6c7361ee5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2001,  6279,  1010,  2026,  9152, 13327,   102,     0,     0],\n",
       "        [  101,  7592,  1010,  2026,  2171,  2003,  7332,  1996, 15214,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Проходя сквозь модель\n",
    "\n",
    "**AutoModel** - загружает только основную нейросетевую архитектуру (BERT, ...), но без специфической \"головы\" (выходного слоя).\n",
    "\n",
    "- Подходит для задач, где нужна эмбеддинговая (скрытая) информация.\n",
    "\n",
    "- Требует добавления выходного слоя вручную при обучении.\n",
    "\n",
    "\n",
    "**AutoModel** вернет только обработанные механизмом внимания эмбеддинги, поэтому используются **AutoModelFor..**, которые и подставляют соответсвующую голову"
   ],
   "id": "44631bb35eb385a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:45.628417Z",
     "start_time": "2025-04-02T14:50:45.218484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ],
   "id": "c65322dd8e74b981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:45.643437Z",
     "start_time": "2025-04-02T14:50:45.629410Z"
    }
   },
   "cell_type": "code",
   "source": "outputs",
   "id": "e4b55ed9da19bf0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.2583,  0.5758, -0.1630,  ..., -0.1796,  0.1406,  0.1123],\n",
       "         [-0.3025,  0.4144,  0.0608,  ...,  0.2308,  0.0462,  0.3716],\n",
       "         [-0.1415,  0.6211, -0.0550,  ...,  0.0570, -0.0231, -0.3812],\n",
       "         ...,\n",
       "         [ 0.1551,  0.2540,  0.0804,  ..., -0.0770, -0.1896, -0.2284],\n",
       "         [-0.0036,  0.5429,  0.3276,  ..., -0.1484,  0.1733,  0.0077],\n",
       "         [-0.1369,  0.5941,  0.0980,  ..., -0.0386,  0.1710,  0.1028]],\n",
       "\n",
       "        [[ 0.1183,  0.5219,  0.3419,  ...,  0.8541,  1.0013,  0.0197],\n",
       "         [ 0.4419,  0.6374,  0.3281,  ...,  0.6219,  1.0461, -0.1147],\n",
       "         [-0.1589,  0.9009,  0.2998,  ...,  0.4126,  0.8258,  0.0569],\n",
       "         ...,\n",
       "         [ 0.2550,  0.1827,  0.5216,  ...,  1.0659,  0.8330,  0.6071],\n",
       "         [-0.1991,  0.0560,  1.0398,  ...,  0.8049,  0.6226, -0.0513],\n",
       "         [ 0.8422,  0.2528,  0.6130,  ...,  0.8830,  0.1598, -0.3166]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Головы моделей: Извлечение смысла из чисел",
   "id": "38a4011ed0ad795e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:46.001642Z",
     "start_time": "2025-04-02T14:50:45.666489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ],
   "id": "aec2c2d534f51835",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6755, -1.4697],\n",
       "        [-2.9174,  3.1364]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Постобработка вывода",
   "id": "7b91723a50abda93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:46.016822Z",
     "start_time": "2025-04-02T14:50:46.003643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import argmax, softmax\n",
    "\n",
    "softmax_predictions = softmax(outputs.logits, dim=-1)\n",
    "argmax_predictions = argmax(outputs.logits, dim=-1)\n",
    "softmax_predictions, argmax_predictions"
   ],
   "id": "834e72e78e3b8a27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9587, 0.0413],\n",
       "         [0.0023, 0.9977]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([0, 1]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Также можно получить лейблы модели",
   "id": "2c7a777e72354559"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:46.265675Z",
     "start_time": "2025-04-02T14:50:46.255Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label",
   "id": "48c3957106472e69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Модели",
   "id": "d724d876fa23645c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AutoModel\n",
    "\n",
    "Скачает модель с весами, но без головы, те возвращать данная модель будет только обработанный механизмом внимания изначальный эмбеддинг"
   ],
   "id": "80a208d1fa9ce6e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:48.383255Z",
     "start_time": "2025-04-02T14:50:46.989803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_model))"
   ],
   "id": "564fd8d8f93642fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartModel'>\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Config\n",
    "\n",
    "Также можно скачать саму структуру модели без весов"
   ],
   "id": "17829965b7db0a12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Можно скачать конфиг (структуру) модели без весов",
   "id": "1f1f21c7e462b2a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:49.098759Z",
     "start_time": "2025-04-02T14:50:48.384255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(\"bert-base-cased\",\n",
    "                                         num_hidden_layers=3)  # Можно менять архитектуру модели, данное изменение ускорит работу, но ухудшит качество\n",
    "print(type(bert_config))\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_config))\n",
    "\n",
    "bart_config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_config))"
   ],
   "id": "2ce026b3fa33aa63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>\n",
      "<class 'transformers.models.bart.configuration_bart.BartConfig'>\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:49.113944Z",
     "start_time": "2025-04-02T14:50:49.098759Z"
    }
   },
   "cell_type": "code",
   "source": "bert_config",
   "id": "924299047d391bb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.50.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "В данные конфиги можно загрузить веса модели",
   "id": "c98ee81262e4fea2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:49.583050Z",
     "start_time": "2025-04-02T14:50:49.116439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\", config=bert_config)\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel.from_pretrained(\"gpt2\", config=gpt_config)\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained(\"facebook/bart-base\", config=bart_config)\n",
    "print(type(bart_model))"
   ],
   "id": "1eb46471208c0b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartModel'>\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Или можно использовать уже подготовленный **ModelConfig**",
   "id": "ba516c2ff6f6729f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:50.484538Z",
     "start_time": "2025-04-02T14:50:49.584138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertConfig, GPT2Config, BartConfig\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_config))\n",
    "\n",
    "gpt_config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_config))\n",
    "\n",
    "bart_config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_config))"
   ],
   "id": "70ca3e30e51e18c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>\n",
      "<class 'transformers.models.bart.configuration_bart.BartConfig'>\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Данные конфиги также можно загрузить в \"оболочку\" модели",
   "id": "8f996d8a961783ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:53.145897Z",
     "start_time": "2025-04-02T14:50:50.487493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel, GPT2Model, BartModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-base\")\n",
    "print(type(bart_model))"
   ],
   "id": "624879ad2a803d6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartModel'>\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving and reloading",
   "id": "e4155ede2db0fe2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:54.018532Z",
     "start_time": "2025-04-02T14:50:53.146427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "bert_model.save_pretrained(\"my-bert-model\")\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\"my-bert-model\")\n",
    "type(bert_model)"
   ],
   "id": "f58896d0303bb0e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "bbc40e64864538a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:54.393892Z",
     "start_time": "2025-04-02T14:50:54.022202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert_model(**bert_tokenizer('zaza boss is me', return_tensors='pt')).last_hidden_state"
   ],
   "id": "38a81736709ea111",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4535,  0.3524,  0.1490,  ..., -0.2225,  0.3883, -0.1498],\n",
       "         [ 0.2127, -0.1671,  0.3893,  ...,  0.4731, -0.5549, -0.0263],\n",
       "         [ 0.5378,  0.3529,  0.4334,  ...,  0.0068,  0.0505,  0.0287],\n",
       "         ...,\n",
       "         [ 0.4239,  0.0934,  0.1885,  ..., -0.1535,  0.0420, -0.1724],\n",
       "         [ 0.4564, -0.1387,  0.2731,  ..., -0.4077,  0.1747, -0.1223],\n",
       "         [ 1.3320,  0.1104,  0.0837,  ..., -0.1996,  1.2236, -0.5436]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Токенизаторы",
   "id": "44729aac440c60b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "У некоторых токенизаторов есть дополнительно поле **token_type_ids**",
   "id": "f777c20a438ff983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:54.891485Z",
     "start_time": "2025-04-02T14:50:54.394883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "inputs = tokenizer('Wasup, my nigger', 'Hello, my name is Ivan the Berg', return_tensors='pt', padding=True,\n",
    "                   truncation=True)\n",
    "inputs"
   ],
   "id": "c986b65ba7970bca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3982,  4455,   117,  1139, 11437,  9146,   102,  8667,   117,\n",
       "          1139,  1271,  1110,  7062,  1103, 16218,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Оно обозначает принадлежность того или иного токена к каждому из переданных предложений. Первое предложение - 0, второе - 1 и так далее",
   "id": "2b12062dd5f249b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.154660Z",
     "start_time": "2025-04-02T14:50:54.893402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "inputs = tokenizer('Wasup, my nigger', 'Hello, my name is Ivan the Berg', return_tensors='pt', padding=True,\n",
    "                   truncation=True)\n",
    "inputs"
   ],
   "id": "81394adcb7d690f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3982,  4455,   117,  1139, 11437,  9146,   102,  8667,   117,\n",
       "          1139,  1271,  1110,  7062,  1103, 16218,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Encoding\n",
    "\n",
    "Перевод текста в числа называется кодированием (encoding). Кодирование выполняется в два этапа: токенизация, а затем преобразование во входные идентификаторы."
   ],
   "id": "85330c578b17c584"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Разбиение текста на токены",
   "id": "5cbafc902d768de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.451204Z",
     "start_time": "2025-04-02T14:50:55.155576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_tokenizer.tokenize(\"Let's try to tokenize!\")"
   ],
   "id": "b7a98b51204e4eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.747426Z",
     "start_time": "2025-04-02T14:50:55.453121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "albert_tokenizer = AutoTokenizer.from_pretrained('albert-base-v1')\n",
    "albert_tokenizer.tokenize(\"Let's try to tokenize!\")"
   ],
   "id": "502d0f5ccfced1f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁let', \"'\", 's', '▁try', '▁to', '▁to', 'ken', 'ize', '!']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Переводим каждый токен в **ID**",
   "id": "8f93a57f9024ae93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.762449Z",
     "start_time": "2025-04-02T14:50:55.747426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_token_ids = bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(\"Let's try to tokenize!\"))\n",
    "bert_token_ids"
   ],
   "id": "9834ec8e0d7913d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2421, 112, 188, 2222, 1106, 22559, 3708, 106]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.777321Z",
     "start_time": "2025-04-02T14:50:55.763445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "albert_token_ids = albert_tokenizer.convert_tokens_to_ids(albert_tokenizer.tokenize(\"Let's try to tokenize!\"))\n",
    "albert_token_ids"
   ],
   "id": "1a6b756db061e537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[408, 22, 18, 1131, 20, 20, 2853, 2952, 187]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Но таким методом мы не учитываем особые токены вида EOS, применим иной метод для этого",
   "id": "c640357ece341173"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.793161Z",
     "start_time": "2025-04-02T14:50:55.778324Z"
    }
   },
   "cell_type": "code",
   "source": "bert_tokenizer.prepare_for_model(bert_token_ids)",
   "id": "10c87ca16d7ace3f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2421, 112, 188, 2222, 1106, 22559, 3708, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.809118Z",
     "start_time": "2025-04-02T14:50:55.794154Z"
    }
   },
   "cell_type": "code",
   "source": "albert_tokenizer.prepare_for_model(albert_token_ids)",
   "id": "da5168834cf75db5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 187, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Посмотрим на обратный метод, что из id сделает токен",
   "id": "926e154baab00d7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.824768Z",
     "start_time": "2025-04-02T14:50:55.810111Z"
    }
   },
   "cell_type": "code",
   "source": "bert_tokenizer.decode(bert_tokenizer.prepare_for_model(bert_token_ids)['input_ids'])",
   "id": "945554af322b7e60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Let ' s try to tokenize! [SEP]\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:55.840890Z",
     "start_time": "2025-04-02T14:50:55.826773Z"
    }
   },
   "cell_type": "code",
   "source": "albert_tokenizer.decode(albert_tokenizer.prepare_for_model(albert_token_ids)['input_ids'])",
   "id": "45aa2ac848f5b4e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] let's try to tokenize![SEP]\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Handling multiple sequences",
   "id": "8caf476255c26f6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:56.447787Z",
     "start_time": "2025-04-02T14:50:55.842882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor(ids)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "# Эта строка выдаст ошибку.\n",
    "model(input_ids)"
   ],
   "id": "4bd5ebd14349bb28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "         2026,  2878,  2166,  1012])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[93], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput IDs:\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_ids)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Эта строка выдаст ошибку.\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:977\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    969\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    970\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m    974\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    975\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 977\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    984\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    986\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[0;32m    987\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:771\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 771\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarn_if_padding_and_no_attention_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    772\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.virtualenvs\\NLP-oFqrAI3X\\lib\\site-packages\\transformers\\modeling_utils.py:5167\u001B[0m, in \u001B[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m   5164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   5166\u001B[0m \u001B[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001B[39;00m\n\u001B[1;32m-> 5167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m   5168\u001B[0m     warn_string \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   5169\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5170\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5171\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5172\u001B[0m     )\n\u001B[0;32m   5174\u001B[0m     \u001B[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001B[39;00m\n\u001B[0;32m   5175\u001B[0m     \u001B[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ошибка сверху вызвана тем, что размерности 1 нам недостаточно",
   "id": "721563af50d7e975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ],
   "id": "39582df3cecfbb76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Добавив ещё одну размерность ошибка была исправлена",
   "id": "2d138704f3d209db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Батчинг** - это отправка нескольких предложений через модель одновременно.",
   "id": "37ab2909dd0cec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]  # выдаст ошибку, тк размеры всех энкодингов должны быть одинаковыми\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "495fe6c427a28d69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Для решения данный проблемы использется **Padding** - добавление пустных значений для достижения одинаковых размеров токенизированных строк",
   "id": "f2bc3359a8a790eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print('Pad Token ID - ', tokenizer.pad_token_id)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id]\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "efee787dd5f240b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Что-то не так с логитами в наших батчах: во втором ряду должны быть те же логиты, что и для второго предложения, но мы получили совершенно другие значения!\n",
    "\n",
    "Это связано с тем, что ключевой особенностью моделей Transformer являются слои внимания (attention layers), которые контекстуализируют каждый токен. Они учитывают токены дополнений, так как рассматривают все токены последовательности. Чтобы получить одинаковый результат при прохождении через модель отдельных предложений разной длины или при прохождении батча с одинаковыми предложениями и дополнениями, нам нужно указать слоям внимания игнорировать дополняющие токены. Для этого используется маска внимания (attention mask)."
   ],
   "id": "ec0415d4caa8a87e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(outputs.logits)"
   ],
   "id": "e0573b733593c11a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Всё это реализовано под капотом библиотеки **transformers**",
   "id": "6abd64126e1d3295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:57.291614Z",
     "start_time": "2025-04-02T14:50:56.986537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer(['Hello, my name is Ivan the Berg', 'I am a nigger'], padding=True)"
   ],
   "id": "5e68307065d9cca0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 1010, 2026, 2171, 2003, 7332, 1996, 15214, 102], [101, 1045, 2572, 1037, 9152, 13327, 102, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Собираём всё воедино",
   "id": "58022aa3b88d74fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:50:59.208020Z",
     "start_time": "2025-04-02T14:50:57.761834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch import softmax\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "texts = ['Hello, my name is Ivan the Berg', 'I am a nigger']\n",
    "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "probs = softmax(output.logits, dim=-1)\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    result = {model.config.id2label[i]: float(prob[i]) for i in range(len(prob))}\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predictions:\", result, '\\n')"
   ],
   "id": "2131421f10171b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello, my name is Ivan the Berg\n",
      "Predictions: {'NEGATIVE': 0.0023433719761669636, 'POSITIVE': 0.9976565837860107} \n",
      "\n",
      "Text: I am a nigger\n",
      "Predictions: {'NEGATIVE': 0.9906907081604004, 'POSITIVE': 0.009309322573244572} \n",
      "\n"
     ]
    }
   ],
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
