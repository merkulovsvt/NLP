{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2743221690590112,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006379585326953748,
      "grad_norm": 4.6808552742004395,
      "learning_rate": 0.0,
      "loss": 2.152,
      "step": 1
    },
    {
      "epoch": 0.012759170653907496,
      "grad_norm": 3.636871576309204,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.1978,
      "step": 2
    },
    {
      "epoch": 0.019138755980861243,
      "grad_norm": 2.0149309635162354,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8641,
      "step": 3
    },
    {
      "epoch": 0.025518341307814992,
      "grad_norm": 2.5643064975738525,
      "learning_rate": 6e-06,
      "loss": 2.2611,
      "step": 4
    },
    {
      "epoch": 0.03189792663476874,
      "grad_norm": 2.5820014476776123,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.8233,
      "step": 5
    },
    {
      "epoch": 0.03827751196172249,
      "grad_norm": 3.184462547302246,
      "learning_rate": 1e-05,
      "loss": 2.2279,
      "step": 6
    },
    {
      "epoch": 0.044657097288676235,
      "grad_norm": 3.938328742980957,
      "learning_rate": 1.2e-05,
      "loss": 2.2611,
      "step": 7
    },
    {
      "epoch": 0.051036682615629984,
      "grad_norm": 2.7864975929260254,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.1682,
      "step": 8
    },
    {
      "epoch": 0.05741626794258373,
      "grad_norm": 2.303053617477417,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3105,
      "step": 9
    },
    {
      "epoch": 0.06379585326953748,
      "grad_norm": 3.3788914680480957,
      "learning_rate": 1.8e-05,
      "loss": 2.3819,
      "step": 10
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 2.0168800354003906,
      "learning_rate": 2e-05,
      "loss": 2.0743,
      "step": 11
    },
    {
      "epoch": 0.07655502392344497,
      "grad_norm": 2.1701722145080566,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.2059,
      "step": 12
    },
    {
      "epoch": 0.08293460925039872,
      "grad_norm": 2.1421773433685303,
      "learning_rate": 2.4e-05,
      "loss": 1.9779,
      "step": 13
    },
    {
      "epoch": 0.08931419457735247,
      "grad_norm": 2.6756577491760254,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 2.4425,
      "step": 14
    },
    {
      "epoch": 0.09569377990430622,
      "grad_norm": 2.9046413898468018,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.9408,
      "step": 15
    },
    {
      "epoch": 0.10207336523125997,
      "grad_norm": 3.6834211349487305,
      "learning_rate": 3e-05,
      "loss": 2.1589,
      "step": 16
    },
    {
      "epoch": 0.10845295055821372,
      "grad_norm": 2.8646650314331055,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.0672,
      "step": 17
    },
    {
      "epoch": 0.11483253588516747,
      "grad_norm": 4.367419242858887,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.5332,
      "step": 18
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 2.5621376037597656,
      "learning_rate": 3.6e-05,
      "loss": 1.9182,
      "step": 19
    },
    {
      "epoch": 0.12759170653907495,
      "grad_norm": 2.0990262031555176,
      "learning_rate": 3.8e-05,
      "loss": 2.2037,
      "step": 20
    },
    {
      "epoch": 0.1339712918660287,
      "grad_norm": 2.3417861461639404,
      "learning_rate": 4e-05,
      "loss": 2.3508,
      "step": 21
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 1.9251339435577393,
      "learning_rate": 4.2e-05,
      "loss": 2.2475,
      "step": 22
    },
    {
      "epoch": 0.1467304625199362,
      "grad_norm": 2.3284568786621094,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.6269,
      "step": 23
    },
    {
      "epoch": 0.15311004784688995,
      "grad_norm": 3.0423176288604736,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.2448,
      "step": 24
    },
    {
      "epoch": 0.1594896331738437,
      "grad_norm": 2.635350465774536,
      "learning_rate": 4.8e-05,
      "loss": 2.1073,
      "step": 25
    },
    {
      "epoch": 0.16586921850079744,
      "grad_norm": 2.262087106704712,
      "learning_rate": 5e-05,
      "loss": 2.5301,
      "step": 26
    },
    {
      "epoch": 0.1722488038277512,
      "grad_norm": 2.135390043258667,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.3774,
      "step": 27
    },
    {
      "epoch": 0.17862838915470494,
      "grad_norm": 2.212146759033203,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.8633,
      "step": 28
    },
    {
      "epoch": 0.1850079744816587,
      "grad_norm": 3.324082374572754,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.1781,
      "step": 29
    },
    {
      "epoch": 0.19138755980861244,
      "grad_norm": 2.2884230613708496,
      "learning_rate": 5.8e-05,
      "loss": 2.1271,
      "step": 30
    },
    {
      "epoch": 0.19776714513556617,
      "grad_norm": 2.172098398208618,
      "learning_rate": 6e-05,
      "loss": 2.244,
      "step": 31
    },
    {
      "epoch": 0.20414673046251994,
      "grad_norm": 2.3516364097595215,
      "learning_rate": 6.2e-05,
      "loss": 1.9972,
      "step": 32
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 1.7785475254058838,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.3463,
      "step": 33
    },
    {
      "epoch": 0.21690590111642744,
      "grad_norm": 1.7681686878204346,
      "learning_rate": 6.6e-05,
      "loss": 2.5336,
      "step": 34
    },
    {
      "epoch": 0.22328548644338117,
      "grad_norm": 2.5022661685943604,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.5563,
      "step": 35
    },
    {
      "epoch": 0.22966507177033493,
      "grad_norm": 1.9184199571609497,
      "learning_rate": 7e-05,
      "loss": 2.26,
      "step": 36
    },
    {
      "epoch": 0.23604465709728867,
      "grad_norm": 2.788785934448242,
      "learning_rate": 7.2e-05,
      "loss": 2.3378,
      "step": 37
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 3.6190223693847656,
      "learning_rate": 7.4e-05,
      "loss": 2.0722,
      "step": 38
    },
    {
      "epoch": 0.24880382775119617,
      "grad_norm": 2.5114192962646484,
      "learning_rate": 7.6e-05,
      "loss": 2.1535,
      "step": 39
    },
    {
      "epoch": 0.2551834130781499,
      "grad_norm": 2.7735884189605713,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.2332,
      "step": 40
    },
    {
      "epoch": 0.26156299840510366,
      "grad_norm": 2.2283496856689453,
      "learning_rate": 8e-05,
      "loss": 2.2786,
      "step": 41
    },
    {
      "epoch": 0.2679425837320574,
      "grad_norm": 1.8890997171401978,
      "learning_rate": 8.2e-05,
      "loss": 2.1665,
      "step": 42
    },
    {
      "epoch": 0.2743221690590112,
      "grad_norm": 2.1425490379333496,
      "learning_rate": 8.4e-05,
      "loss": 2.2247,
      "step": 43
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 2.836578607559204,
      "learning_rate": 8.6e-05,
      "loss": 1.8911,
      "step": 44
    },
    {
      "epoch": 0.28708133971291866,
      "grad_norm": 2.090292453765869,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.736,
      "step": 45
    },
    {
      "epoch": 0.2934609250398724,
      "grad_norm": 2.15342378616333,
      "learning_rate": 9e-05,
      "loss": 2.0354,
      "step": 46
    },
    {
      "epoch": 0.29984051036682613,
      "grad_norm": 2.8733787536621094,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.7909,
      "step": 47
    },
    {
      "epoch": 0.3062200956937799,
      "grad_norm": 3.2691147327423096,
      "learning_rate": 9.4e-05,
      "loss": 2.0668,
      "step": 48
    },
    {
      "epoch": 0.31259968102073366,
      "grad_norm": 2.9228672981262207,
      "learning_rate": 9.6e-05,
      "loss": 1.9149,
      "step": 49
    },
    {
      "epoch": 0.3189792663476874,
      "grad_norm": 2.3718206882476807,
      "learning_rate": 9.8e-05,
      "loss": 1.9279,
      "step": 50
    },
    {
      "epoch": 0.3253588516746411,
      "grad_norm": 1.6053595542907715,
      "learning_rate": 0.0001,
      "loss": 2.2314,
      "step": 51
    },
    {
      "epoch": 0.3317384370015949,
      "grad_norm": 2.0921030044555664,
      "learning_rate": 0.00010200000000000001,
      "loss": 2.084,
      "step": 52
    },
    {
      "epoch": 0.33811802232854865,
      "grad_norm": 2.640717029571533,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.5433,
      "step": 53
    },
    {
      "epoch": 0.3444976076555024,
      "grad_norm": 2.3015897274017334,
      "learning_rate": 0.00010600000000000002,
      "loss": 1.5589,
      "step": 54
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 1.8687468767166138,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.8165,
      "step": 55
    },
    {
      "epoch": 0.3572567783094099,
      "grad_norm": NaN,
      "learning_rate": 0.00011000000000000002,
      "loss": 1.6765,
      "step": 56
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 2.3182811737060547,
      "learning_rate": 0.00011000000000000002,
      "loss": 2.2756,
      "step": 57
    },
    {
      "epoch": 0.3700159489633174,
      "grad_norm": 2.8479135036468506,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.4757,
      "step": 58
    },
    {
      "epoch": 0.3763955342902711,
      "grad_norm": 3.809335470199585,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.8447,
      "step": 59
    },
    {
      "epoch": 0.3827751196172249,
      "grad_norm": 1.8485867977142334,
      "learning_rate": 0.000116,
      "loss": 1.6437,
      "step": 60
    },
    {
      "epoch": 0.38915470494417864,
      "grad_norm": 3.01426100730896,
      "learning_rate": 0.000118,
      "loss": 2.2445,
      "step": 61
    },
    {
      "epoch": 0.39553429027113235,
      "grad_norm": 2.116159200668335,
      "learning_rate": 0.00012,
      "loss": 1.5485,
      "step": 62
    },
    {
      "epoch": 0.4019138755980861,
      "grad_norm": 1.696394681930542,
      "learning_rate": 0.000122,
      "loss": 2.1048,
      "step": 63
    },
    {
      "epoch": 0.4082934609250399,
      "grad_norm": 1.8328441381454468,
      "learning_rate": 0.000124,
      "loss": 1.9302,
      "step": 64
    },
    {
      "epoch": 0.41467304625199364,
      "grad_norm": 1.9863463640213013,
      "learning_rate": 0.000126,
      "loss": 1.9962,
      "step": 65
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 2.1045243740081787,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.9328,
      "step": 66
    },
    {
      "epoch": 0.4274322169059011,
      "grad_norm": 2.3743183612823486,
      "learning_rate": 0.00013000000000000002,
      "loss": 1.957,
      "step": 67
    },
    {
      "epoch": 0.43381180223285487,
      "grad_norm": 1.934859275817871,
      "learning_rate": 0.000132,
      "loss": 2.2833,
      "step": 68
    },
    {
      "epoch": 0.44019138755980863,
      "grad_norm": 2.4314022064208984,
      "learning_rate": 0.000134,
      "loss": 2.0687,
      "step": 69
    },
    {
      "epoch": 0.44657097288676234,
      "grad_norm": 2.7309184074401855,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.8856,
      "step": 70
    },
    {
      "epoch": 0.4529505582137161,
      "grad_norm": 1.9753384590148926,
      "learning_rate": 0.000138,
      "loss": 1.5709,
      "step": 71
    },
    {
      "epoch": 0.45933014354066987,
      "grad_norm": 2.3598580360412598,
      "learning_rate": 0.00014,
      "loss": 2.4132,
      "step": 72
    },
    {
      "epoch": 0.46570972886762363,
      "grad_norm": 2.834239959716797,
      "learning_rate": 0.000142,
      "loss": 1.7432,
      "step": 73
    },
    {
      "epoch": 0.47208931419457734,
      "grad_norm": 2.2765302658081055,
      "learning_rate": 0.000144,
      "loss": 2.2373,
      "step": 74
    },
    {
      "epoch": 0.4784688995215311,
      "grad_norm": 1.8989241123199463,
      "learning_rate": 0.000146,
      "loss": 1.8079,
      "step": 75
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.9912328720092773,
      "learning_rate": 0.000148,
      "loss": 1.863,
      "step": 76
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 2.6389756202697754,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.5518,
      "step": 77
    },
    {
      "epoch": 0.49760765550239233,
      "grad_norm": 2.4546422958374023,
      "learning_rate": 0.000152,
      "loss": 1.7979,
      "step": 78
    },
    {
      "epoch": 0.5039872408293461,
      "grad_norm": 3.463076114654541,
      "learning_rate": 0.000154,
      "loss": 2.1505,
      "step": 79
    },
    {
      "epoch": 0.5103668261562998,
      "grad_norm": 2.019674062728882,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.1618,
      "step": 80
    },
    {
      "epoch": 0.5167464114832536,
      "grad_norm": 2.910521984100342,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.8909,
      "step": 81
    },
    {
      "epoch": 0.5231259968102073,
      "grad_norm": 2.010326385498047,
      "learning_rate": 0.00016,
      "loss": 2.0269,
      "step": 82
    },
    {
      "epoch": 0.529505582137161,
      "grad_norm": 3.707289695739746,
      "learning_rate": 0.000162,
      "loss": 1.7329,
      "step": 83
    },
    {
      "epoch": 0.5358851674641149,
      "grad_norm": 1.9385467767715454,
      "learning_rate": 0.000164,
      "loss": 1.948,
      "step": 84
    },
    {
      "epoch": 0.5422647527910686,
      "grad_norm": 1.992560863494873,
      "learning_rate": 0.000166,
      "loss": 2.1667,
      "step": 85
    },
    {
      "epoch": 0.5486443381180224,
      "grad_norm": 2.5643844604492188,
      "learning_rate": 0.000168,
      "loss": 2.0964,
      "step": 86
    },
    {
      "epoch": 0.5550239234449761,
      "grad_norm": 2.5558815002441406,
      "learning_rate": 0.00017,
      "loss": 2.3109,
      "step": 87
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 2.6272761821746826,
      "learning_rate": 0.000172,
      "loss": 1.8441,
      "step": 88
    },
    {
      "epoch": 0.5677830940988836,
      "grad_norm": 1.601408839225769,
      "learning_rate": 0.000174,
      "loss": 2.1182,
      "step": 89
    },
    {
      "epoch": 0.5741626794258373,
      "grad_norm": 2.1516714096069336,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.5459,
      "step": 90
    },
    {
      "epoch": 0.580542264752791,
      "grad_norm": 2.0481467247009277,
      "learning_rate": 0.00017800000000000002,
      "loss": 2.007,
      "step": 91
    },
    {
      "epoch": 0.5869218500797448,
      "grad_norm": 1.5913612842559814,
      "learning_rate": 0.00018,
      "loss": 1.5708,
      "step": 92
    },
    {
      "epoch": 0.5933014354066986,
      "grad_norm": 2.2769017219543457,
      "learning_rate": 0.000182,
      "loss": 1.8431,
      "step": 93
    },
    {
      "epoch": 0.5996810207336523,
      "grad_norm": 2.1401753425598145,
      "learning_rate": 0.00018400000000000003,
      "loss": 2.1293,
      "step": 94
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.816338300704956,
      "learning_rate": 0.00018600000000000002,
      "loss": 2.1942,
      "step": 95
    },
    {
      "epoch": 0.6124401913875598,
      "grad_norm": 1.3922992944717407,
      "learning_rate": 0.000188,
      "loss": 2.3774,
      "step": 96
    },
    {
      "epoch": 0.6188197767145136,
      "grad_norm": 1.6783772706985474,
      "learning_rate": 0.00019,
      "loss": 2.0754,
      "step": 97
    },
    {
      "epoch": 0.6251993620414673,
      "grad_norm": 2.3557727336883545,
      "learning_rate": 0.000192,
      "loss": 2.255,
      "step": 98
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 2.050347089767456,
      "learning_rate": 0.000194,
      "loss": 1.779,
      "step": 99
    },
    {
      "epoch": 0.6379585326953748,
      "grad_norm": 2.1268153190612793,
      "learning_rate": 0.000196,
      "loss": 1.7993,
      "step": 100
    },
    {
      "epoch": 0.6443381180223285,
      "grad_norm": 2.7106475830078125,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.87,
      "step": 101
    },
    {
      "epoch": 0.6507177033492823,
      "grad_norm": 1.0856153964996338,
      "learning_rate": 0.0002,
      "loss": 2.1891,
      "step": 102
    },
    {
      "epoch": 0.6570972886762361,
      "grad_norm": 1.8516656160354614,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.9468,
      "step": 103
    },
    {
      "epoch": 0.6634768740031898,
      "grad_norm": 2.2672336101531982,
      "learning_rate": 0.000196,
      "loss": 2.1459,
      "step": 104
    },
    {
      "epoch": 0.6698564593301436,
      "grad_norm": 2.2405436038970947,
      "learning_rate": 0.000194,
      "loss": 1.4942,
      "step": 105
    },
    {
      "epoch": 0.6762360446570973,
      "grad_norm": 2.0778729915618896,
      "learning_rate": 0.000192,
      "loss": 1.9217,
      "step": 106
    },
    {
      "epoch": 0.682615629984051,
      "grad_norm": 1.9874560832977295,
      "learning_rate": 0.00019,
      "loss": 2.1864,
      "step": 107
    },
    {
      "epoch": 0.6889952153110048,
      "grad_norm": 2.1291818618774414,
      "learning_rate": 0.000188,
      "loss": 2.1826,
      "step": 108
    },
    {
      "epoch": 0.6953748006379585,
      "grad_norm": 1.78303861618042,
      "learning_rate": 0.00018600000000000002,
      "loss": 1.6749,
      "step": 109
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 2.144005298614502,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.8491,
      "step": 110
    },
    {
      "epoch": 0.7081339712918661,
      "grad_norm": 1.3326276540756226,
      "learning_rate": 0.000182,
      "loss": 2.1094,
      "step": 111
    },
    {
      "epoch": 0.7145135566188198,
      "grad_norm": 1.1697226762771606,
      "learning_rate": 0.00018,
      "loss": 2.0855,
      "step": 112
    },
    {
      "epoch": 0.7208931419457735,
      "grad_norm": 2.381833553314209,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.8513,
      "step": 113
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.6306527853012085,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.3219,
      "step": 114
    },
    {
      "epoch": 0.733652312599681,
      "grad_norm": 1.305921196937561,
      "learning_rate": 0.000174,
      "loss": 2.416,
      "step": 115
    },
    {
      "epoch": 0.7400318979266348,
      "grad_norm": 1.486857295036316,
      "learning_rate": 0.000172,
      "loss": 2.1411,
      "step": 116
    },
    {
      "epoch": 0.7464114832535885,
      "grad_norm": 2.2641472816467285,
      "learning_rate": 0.00017,
      "loss": 1.7484,
      "step": 117
    },
    {
      "epoch": 0.7527910685805422,
      "grad_norm": 2.076695203781128,
      "learning_rate": 0.000168,
      "loss": 1.9157,
      "step": 118
    },
    {
      "epoch": 0.759170653907496,
      "grad_norm": 1.986559271812439,
      "learning_rate": 0.000166,
      "loss": 2.0416,
      "step": 119
    },
    {
      "epoch": 0.7655502392344498,
      "grad_norm": 2.0065579414367676,
      "learning_rate": 0.000164,
      "loss": 2.0544,
      "step": 120
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 1.9687378406524658,
      "learning_rate": 0.000162,
      "loss": 2.0764,
      "step": 121
    },
    {
      "epoch": 0.7783094098883573,
      "grad_norm": 3.329956531524658,
      "learning_rate": 0.00016,
      "loss": 2.1902,
      "step": 122
    },
    {
      "epoch": 0.784688995215311,
      "grad_norm": 1.8989002704620361,
      "learning_rate": 0.00015800000000000002,
      "loss": 2.159,
      "step": 123
    },
    {
      "epoch": 0.7910685805422647,
      "grad_norm": 1.9204294681549072,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.144,
      "step": 124
    },
    {
      "epoch": 0.7974481658692185,
      "grad_norm": 2.380654811859131,
      "learning_rate": 0.000154,
      "loss": 1.8849,
      "step": 125
    },
    {
      "epoch": 0.8038277511961722,
      "grad_norm": 2.108593225479126,
      "learning_rate": 0.000152,
      "loss": 2.2164,
      "step": 126
    },
    {
      "epoch": 0.810207336523126,
      "grad_norm": 2.191845178604126,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.7329,
      "step": 127
    },
    {
      "epoch": 0.8165869218500797,
      "grad_norm": 1.431928277015686,
      "learning_rate": 0.000148,
      "loss": 2.1232,
      "step": 128
    },
    {
      "epoch": 0.8229665071770335,
      "grad_norm": 1.9380977153778076,
      "learning_rate": 0.000146,
      "loss": 2.2667,
      "step": 129
    },
    {
      "epoch": 0.8293460925039873,
      "grad_norm": 1.420838475227356,
      "learning_rate": 0.000144,
      "loss": 2.2914,
      "step": 130
    },
    {
      "epoch": 0.835725677830941,
      "grad_norm": 1.7441297769546509,
      "learning_rate": 0.000142,
      "loss": 1.9953,
      "step": 131
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 1.9871602058410645,
      "learning_rate": 0.00014,
      "loss": 2.1082,
      "step": 132
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 2.081129550933838,
      "learning_rate": 0.000138,
      "loss": 2.2398,
      "step": 133
    },
    {
      "epoch": 0.8548644338118022,
      "grad_norm": 2.2914552688598633,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.0379,
      "step": 134
    },
    {
      "epoch": 0.861244019138756,
      "grad_norm": 1.669999361038208,
      "learning_rate": 0.000134,
      "loss": 2.0236,
      "step": 135
    },
    {
      "epoch": 0.8676236044657097,
      "grad_norm": 2.6715188026428223,
      "learning_rate": 0.000132,
      "loss": 2.0776,
      "step": 136
    },
    {
      "epoch": 0.8740031897926634,
      "grad_norm": 1.9816454648971558,
      "learning_rate": 0.00013000000000000002,
      "loss": 2.0927,
      "step": 137
    },
    {
      "epoch": 0.8803827751196173,
      "grad_norm": 1.9217684268951416,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.3956,
      "step": 138
    },
    {
      "epoch": 0.886762360446571,
      "grad_norm": 1.841367483139038,
      "learning_rate": 0.000126,
      "loss": 2.1565,
      "step": 139
    },
    {
      "epoch": 0.8931419457735247,
      "grad_norm": 1.6538519859313965,
      "learning_rate": 0.000124,
      "loss": 1.9163,
      "step": 140
    },
    {
      "epoch": 0.8995215311004785,
      "grad_norm": 2.131824254989624,
      "learning_rate": 0.000122,
      "loss": 2.007,
      "step": 141
    },
    {
      "epoch": 0.9059011164274322,
      "grad_norm": 2.1183111667633057,
      "learning_rate": 0.00012,
      "loss": 2.3143,
      "step": 142
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 1.668157696723938,
      "learning_rate": 0.000118,
      "loss": 2.3796,
      "step": 143
    },
    {
      "epoch": 0.9186602870813397,
      "grad_norm": 2.313582181930542,
      "learning_rate": 0.000116,
      "loss": 2.1398,
      "step": 144
    },
    {
      "epoch": 0.9250398724082934,
      "grad_norm": 2.7401294708251953,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.7462,
      "step": 145
    },
    {
      "epoch": 0.9314194577352473,
      "grad_norm": 1.6569589376449585,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.3146,
      "step": 146
    },
    {
      "epoch": 0.937799043062201,
      "grad_norm": 2.0930447578430176,
      "learning_rate": 0.00011000000000000002,
      "loss": 1.851,
      "step": 147
    },
    {
      "epoch": 0.9441786283891547,
      "grad_norm": 1.6983908414840698,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.1297,
      "step": 148
    },
    {
      "epoch": 0.9505582137161085,
      "grad_norm": 3.156278610229492,
      "learning_rate": 0.00010600000000000002,
      "loss": 1.7607,
      "step": 149
    },
    {
      "epoch": 0.9569377990430622,
      "grad_norm": 1.8258010149002075,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.8628,
      "step": 150
    },
    {
      "epoch": 0.9633173843700159,
      "grad_norm": 1.847202181816101,
      "learning_rate": 0.00010200000000000001,
      "loss": 2.0329,
      "step": 151
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 1.7355972528457642,
      "learning_rate": 0.0001,
      "loss": 1.428,
      "step": 152
    },
    {
      "epoch": 0.9760765550239234,
      "grad_norm": 1.783293604850769,
      "learning_rate": 9.8e-05,
      "loss": 1.7089,
      "step": 153
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 2.3104465007781982,
      "learning_rate": 9.6e-05,
      "loss": 2.2829,
      "step": 154
    },
    {
      "epoch": 0.988835725677831,
      "grad_norm": 1.9084765911102295,
      "learning_rate": 9.4e-05,
      "loss": 1.7525,
      "step": 155
    },
    {
      "epoch": 0.9952153110047847,
      "grad_norm": 2.0977187156677246,
      "learning_rate": 9.200000000000001e-05,
      "loss": 2.4271,
      "step": 156
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9755072593688965,
      "learning_rate": 9e-05,
      "loss": 2.2746,
      "step": 157
    },
    {
      "epoch": 1.0063795853269537,
      "grad_norm": 1.6992923021316528,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.7343,
      "step": 158
    },
    {
      "epoch": 1.0127591706539074,
      "grad_norm": 1.5442434549331665,
      "learning_rate": 8.6e-05,
      "loss": 1.5643,
      "step": 159
    },
    {
      "epoch": 1.0191387559808613,
      "grad_norm": 2.096061944961548,
      "learning_rate": 8.4e-05,
      "loss": 1.8436,
      "step": 160
    },
    {
      "epoch": 1.025518341307815,
      "grad_norm": 2.0053234100341797,
      "learning_rate": 8.2e-05,
      "loss": 1.9482,
      "step": 161
    },
    {
      "epoch": 1.0318979266347688,
      "grad_norm": 1.9360979795455933,
      "learning_rate": 8e-05,
      "loss": 2.0235,
      "step": 162
    },
    {
      "epoch": 1.0382775119617225,
      "grad_norm": 2.684389114379883,
      "learning_rate": 7.800000000000001e-05,
      "loss": 1.6829,
      "step": 163
    },
    {
      "epoch": 1.0446570972886762,
      "grad_norm": 2.1170969009399414,
      "learning_rate": 7.6e-05,
      "loss": 2.0385,
      "step": 164
    },
    {
      "epoch": 1.0510366826156299,
      "grad_norm": 1.8704921007156372,
      "learning_rate": 7.4e-05,
      "loss": 2.1675,
      "step": 165
    },
    {
      "epoch": 1.0574162679425838,
      "grad_norm": 1.8447074890136719,
      "learning_rate": 7.2e-05,
      "loss": 2.0336,
      "step": 166
    },
    {
      "epoch": 1.0637958532695375,
      "grad_norm": 2.2416818141937256,
      "learning_rate": 7e-05,
      "loss": 1.9725,
      "step": 167
    },
    {
      "epoch": 1.0701754385964912,
      "grad_norm": 2.5022146701812744,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.0626,
      "step": 168
    },
    {
      "epoch": 1.076555023923445,
      "grad_norm": 1.9784460067749023,
      "learning_rate": 6.6e-05,
      "loss": 2.1761,
      "step": 169
    },
    {
      "epoch": 1.0829346092503986,
      "grad_norm": 2.262512445449829,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.5443,
      "step": 170
    },
    {
      "epoch": 1.0893141945773526,
      "grad_norm": 2.2281365394592285,
      "learning_rate": 6.2e-05,
      "loss": 1.7518,
      "step": 171
    },
    {
      "epoch": 1.0956937799043063,
      "grad_norm": 2.442220449447632,
      "learning_rate": 6e-05,
      "loss": 1.5949,
      "step": 172
    },
    {
      "epoch": 1.10207336523126,
      "grad_norm": 1.6045607328414917,
      "learning_rate": 5.8e-05,
      "loss": 1.9815,
      "step": 173
    },
    {
      "epoch": 1.1084529505582137,
      "grad_norm": 3.793454885482788,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.1915,
      "step": 174
    },
    {
      "epoch": 1.1148325358851674,
      "grad_norm": 2.2347638607025146,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.8824,
      "step": 175
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 2.2281763553619385,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.0839,
      "step": 176
    },
    {
      "epoch": 1.127591706539075,
      "grad_norm": 1.8468639850616455,
      "learning_rate": 5e-05,
      "loss": 1.916,
      "step": 177
    },
    {
      "epoch": 1.1339712918660287,
      "grad_norm": 1.4698259830474854,
      "learning_rate": 4.8e-05,
      "loss": 2.2464,
      "step": 178
    },
    {
      "epoch": 1.1403508771929824,
      "grad_norm": 1.8297652006149292,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.8403,
      "step": 179
    },
    {
      "epoch": 1.1467304625199362,
      "grad_norm": 1.9113889932632446,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.1582,
      "step": 180
    },
    {
      "epoch": 1.1531100478468899,
      "grad_norm": 1.9225424528121948,
      "learning_rate": 4.2e-05,
      "loss": 1.9335,
      "step": 181
    },
    {
      "epoch": 1.1594896331738438,
      "grad_norm": 1.781838059425354,
      "learning_rate": 4e-05,
      "loss": 2.1836,
      "step": 182
    },
    {
      "epoch": 1.1658692185007975,
      "grad_norm": 2.2820818424224854,
      "learning_rate": 3.8e-05,
      "loss": 1.9874,
      "step": 183
    },
    {
      "epoch": 1.1722488038277512,
      "grad_norm": 1.7243309020996094,
      "learning_rate": 3.6e-05,
      "loss": 1.9193,
      "step": 184
    },
    {
      "epoch": 1.178628389154705,
      "grad_norm": 1.6014388799667358,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.8208,
      "step": 185
    },
    {
      "epoch": 1.1850079744816586,
      "grad_norm": 1.6348334550857544,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8231,
      "step": 186
    },
    {
      "epoch": 1.1913875598086126,
      "grad_norm": 2.1852684020996094,
      "learning_rate": 3e-05,
      "loss": 2.387,
      "step": 187
    },
    {
      "epoch": 1.1977671451355663,
      "grad_norm": 1.4532043933868408,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.1766,
      "step": 188
    },
    {
      "epoch": 1.20414673046252,
      "grad_norm": 2.3139352798461914,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 2.0079,
      "step": 189
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 2.807373285293579,
      "learning_rate": 2.4e-05,
      "loss": 1.6241,
      "step": 190
    },
    {
      "epoch": 1.2169059011164274,
      "grad_norm": 1.7749677896499634,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.4657,
      "step": 191
    },
    {
      "epoch": 1.223285486443381,
      "grad_norm": 1.7660982608795166,
      "learning_rate": 2e-05,
      "loss": 2.1712,
      "step": 192
    },
    {
      "epoch": 1.229665071770335,
      "grad_norm": 1.4801063537597656,
      "learning_rate": 1.8e-05,
      "loss": 2.1681,
      "step": 193
    },
    {
      "epoch": 1.2360446570972887,
      "grad_norm": 2.183741331100464,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.73,
      "step": 194
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 2.1579442024230957,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.2413,
      "step": 195
    },
    {
      "epoch": 1.2488038277511961,
      "grad_norm": 2.6064815521240234,
      "learning_rate": 1.2e-05,
      "loss": 1.9874,
      "step": 196
    },
    {
      "epoch": 1.2551834130781498,
      "grad_norm": 1.3101613521575928,
      "learning_rate": 1e-05,
      "loss": 2.511,
      "step": 197
    },
    {
      "epoch": 1.2615629984051036,
      "grad_norm": 1.2522021532058716,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.1514,
      "step": 198
    },
    {
      "epoch": 1.2679425837320575,
      "grad_norm": 2.540524482727051,
      "learning_rate": 6e-06,
      "loss": 1.9715,
      "step": 199
    },
    {
      "epoch": 1.2743221690590112,
      "grad_norm": 1.926463007926941,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.7839,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1453832390123520.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
